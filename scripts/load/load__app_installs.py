from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date
import argparse
import os

parser = argparse.ArgumentParser()
parser.add_argument('--jdbc-url', required=True)
parser.add_argument('--db-user', required=True)
parser.add_argument('--db-password', required=True)
parser.add_argument('--table-name', required=True)
parser.add_argument('--s3-path', required=True)

args = parser.parse_args()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
spark = SparkSession.builder \
    .appName("JdbcToS3AppInstalls") \
    .config("spark.ui.port", "4041") \
    .getOrCreate()

# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
s3_data_path = args.s3_path
s3_checkpoint_path = os.path.join(s3_data_path, "_checkpoint")

# –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ñ–∞–π–ª—ã –≤ S3
try:
    existing_data_df = spark.read.parquet(s3_data_path)
    max_ts = existing_data_df.selectExpr("MAX(ts) as max_ts").collect()[0]["max_ts"]
    print(f"üîÅ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å ts > {max_ts}")
    predicate = f"ts > timestamp '{max_ts}'"
except Exception as e:
    print("üÜï –î–∞–Ω–Ω—ã—Ö –≤ S3 –Ω–µ—Ç, –∑–∞–≥—Ä—É–∑–∏–º –≤—Å—ë –∏–∑ –±–∞–∑—ã.")
    predicate = "1=1"

# –ß—Ç–µ–Ω–∏–µ –∏–∑ PostgreSQL
jdbc_df = spark.read \
    .format("jdbc") \
    .option("url", args.jdbc_url) \
    .option("user", args.db_user) \
    .option("password", args.db_password) \
    .option("dbtable", args.table_name) \
    .option("fetchsize", 1000) \
    .option("driver", "org.postgresql.Driver") \
    .option("pushDownPredicate", "true") \
    .load() \
    .filter(predicate)

# –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞—Ç–æ–π –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
df_with_partition = jdbc_df \
    .withColumn("event_date", to_date(col("ts")))

# –ó–∞–ø–∏—Å—å –≤ S3 —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
df_with_partition.write \
    .mode("append") \
    .partitionBy("event_date") \
    .parquet(s3_data_path)

print("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
